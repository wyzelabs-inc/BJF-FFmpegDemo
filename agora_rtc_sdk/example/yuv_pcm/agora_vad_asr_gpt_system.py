#!/usr/bin/env python3
"""
Agora VAD+ASR+GPT ä¸€ä½“åŒ–å¯¹è¯ç³»ç»Ÿ

é…ç½®è¯´æ˜:
è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY:
export OPENAI_API_KEY="your-openai-api-key-here"

å¦‚æœæœªè®¾ç½®APIå¯†é’¥ï¼Œç³»ç»Ÿå°†ä½¿ç”¨æ¨¡æ‹Ÿå›å¤æ¨¡å¼
"""

import os
import time

import threading
import numpy as np
import ctypes
from typing import Optional, Callable
from collections import deque
from enum import Enum

# æ£€æŸ¥ä¾èµ–
try:
    import torch
    TORCH_AVAILABLE = True
    print("âœ… PyTorchå¯ç”¨")
except ImportError:
    TORCH_AVAILABLE = False
    print("âŒ PyTorchä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–VAD")

try:
    import sherpa_onnx
    SHERPA_AVAILABLE = True
    print("âœ… Sherpa-ONNXå¯ç”¨")
except ImportError:
    SHERPA_AVAILABLE = False
    print("âŒ Sherpa-ONNXä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–ASR")

try:
    import openai
    OPENAI_AVAILABLE = True
    print("âœ… OpenAIå¯ç”¨")
except ImportError:
    OPENAI_AVAILABLE = False
    print("âŒ OpenAIä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå›å¤")

# å¯¹è¯çŠ¶æ€æšä¸¾
class ConversationState(Enum):
    LISTENING = "listening"      # ç›‘å¬è¯­éŸ³
    RECOGNIZING = "recognizing"  # è¯­éŸ³è¯†åˆ«ä¸­
    CHATTING = "chatting"       # GPTæ€è€ƒä¸­
    SPEAKING = "speaking"       # æ’­æ”¾å›å¤ä¸­

class SileroVAD:
    """Silero VADè¯­éŸ³æ´»åŠ¨æ£€æµ‹"""
    
    def __init__(self, model_path: str, threshold: float = 0.04, threshold_low: float = 0.005):
        self.threshold = threshold # è¿›ä¸€æ­¥æé«˜é˜ˆå€¼ä»¥é™ä½æ•æ„Ÿåº¦
        self.threshold_low = threshold_low
        self.sample_rate = 16000
        self.frame_size = 512  # 32ms @ 16kHz
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ”„ åŠ è½½Silero VADæ¨¡å‹: {model_path}")
        self.model, _ = torch.hub.load(
            repo_or_dir=model_path,
            model='silero_vad',
            source='local',
            force_reload=False,
            onnx=False
        )
        self.model.eval()
        print("âœ… Silero VADæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # çŠ¶æ€å˜é‡
        self.audio_buffer = []
        self.last_is_voice = False
    
    def process_audio(self, audio_chunk: np.ndarray) -> bool:
        """å¤„ç†éŸ³é¢‘å—ï¼Œè¿”å›æ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³"""
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.audio_buffer.extend(audio_chunk)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡ŒVAD
        if len(self.audio_buffer) < self.frame_size:
            return self.last_is_voice
        
        # æå–ä¸€å¸§æ•°æ®
        frame = np.array(self.audio_buffer[:self.frame_size], dtype=np.float32)
        self.audio_buffer = self.audio_buffer[self.frame_size:]
        
        # VADæ£€æµ‹
        with torch.no_grad():
            frame_tensor = torch.from_numpy(frame)
            speech_prob = self.model(frame_tensor, self.sample_rate).item()
        
        # å†³ç­–é€»è¾‘
        if speech_prob >= self.threshold:
            is_voice = True
        elif speech_prob <= self.threshold_low:
            is_voice = False
        else:
            is_voice = self.last_is_voice  # ä¿æŒä¸Šä¸€çŠ¶æ€
        
        self.last_is_voice = is_voice
        return is_voice

class SherpaOnnxASR:
    """Sherpa-ONNX ASRè¯­éŸ³è¯†åˆ«"""
    
    def __init__(self, model_dir: str):
        self.model_dir = model_dir
        print(f"æ¨¡å‹ç›®å½•: {model_dir}")
        
        # åˆå§‹åŒ–è¯†åˆ«å™¨
        print("æ­£åœ¨åˆå§‹åŒ– Whisper æ¨¡å‹...")
        recognizer = sherpa_onnx.OfflineRecognizer.from_whisper(
            encoder=f"{model_dir}/base.en-encoder.onnx",
            decoder=f"{model_dir}/base.en-decoder.onnx",
            tokens=f"{model_dir}/base.en-tokens.txt"
        )
        self.recognizer = recognizer
        print("æ¨¡å‹åˆå§‹åŒ–å®Œæˆ!")
    
    def transcribe_audio_data(self, audio_data: np.ndarray, sample_rate: int) -> str:
        """ç›´æ¥ä»numpyæ•°ç»„è¯†åˆ«è¯­éŸ³"""
        try:
            # åˆ›å»ºéŸ³é¢‘æµ
            stream = self.recognizer.create_stream()
            
            # è¾“å…¥éŸ³é¢‘æ•°æ®
            stream.accept_waveform(sample_rate, audio_data)
            
            # æ‰§è¡Œè¯†åˆ«
            self.recognizer.decode_stream(stream)
            result = stream.result.text.strip()
            
            return result
            
        except Exception as e:
            print(f"âŒ ASRè¯†åˆ«é”™è¯¯: {e}")
            return ""

class GPTClient:
    """GPTå¯¹è¯å®¢æˆ·ç«¯"""
    
    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        api_key = os.getenv('OPENAI_API_KEY')

        if api_key:
            openai.api_key = api_key
            print("âœ… GPTå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            self.use_mock = False
        else:
            print("âš ï¸  æœªè®¾ç½®OpenAI APIå¯†é’¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå›å¤")
            self.use_mock = True
        
        # å¯¹è¯å†å²
        self.conversation_history = []
    
    def chat(self, user_message: str) -> str:
        """ä¸GPTå¯¹è¯"""
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.conversation_history.append({"role": "user", "content": user_message})

            # ä¿æŒå†å²é•¿åº¦
            if len(self.conversation_history) > 10:
                self.conversation_history = self.conversation_history[-10:]

            # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
            client = openai.OpenAI(api_key=openai.api_key)

            # è°ƒç”¨GPT API (æ–°ç‰ˆæœ¬è¯­æ³•)
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "you are a friendly ai robot,response my question"},
                    *self.conversation_history
                ],
                max_tokens=150,
                temperature=0.7
            )

            reply = response.choices[0].message.content.strip()

            # æ·»åŠ AIå›å¤åˆ°å†å²
            self.conversation_history.append({"role": "assistant", "content": reply})

            return reply

        except Exception as e:
            print(f"âŒ GPT APIé”™è¯¯: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›å¤ã€‚"

class AgoraConversationSystem:
    """Agoraå¯¹è¯ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self,
                 asr_model_dir: str = "../models/sherpa-onnx-whisper-base.en",
                 vad_model_dir: str = "../models/snakers4_silero-vad",
                 vad_threshold: float = 0.1, # ç»Ÿä¸€é»˜è®¤å€¼ä¸º0.1
                 vad_threshold_low: float = 0.005,
                 sample_rate: int = 16000):
        
        self.sample_rate = sample_rate
        
        # åˆå§‹åŒ–ç»„ä»¶
        if TORCH_AVAILABLE:
            self.vad = SileroVAD(vad_model_dir, vad_threshold, vad_threshold_low)
        else:
            raise RuntimeError("éœ€è¦PyTorchæ”¯æŒ")
        
        if SHERPA_AVAILABLE:
            self.asr = SherpaOnnxASR(asr_model_dir)
        else:
            raise RuntimeError("éœ€è¦Sherpa-ONNXæ”¯æŒ")
        
        self.gpt = GPTClient()
        
        # çŠ¶æ€ç®¡ç†
        self.state = ConversationState.LISTENING
        self.state_lock = threading.Lock()
        
        # VADçŠ¶æ€æœº
        self.is_speaking = False
        self.speech_chunk_count = 0
        self.silence_chunk_count = 0
        self.min_speech_chunks = 7      # æœ€å°è¿ç»­è¯­éŸ³å—æ•° (7å—=224ms)
        self.max_silence_chunks = 15    # æœ€å¤§è¿ç»­é™éŸ³å—æ•° (15å—=480ms)ï¼Œå¢åŠ æ­¤å€¼å¯å®¹å¿æ›´é•¿çš„åœé¡¿
        
        # è¯­éŸ³æ”¶é›†
        self.is_collecting_speech = False
        self.speech_buffer = []

        # å‰ç½®ç¼“å†²åŒºï¼Œç”¨äºæ•è·è¯­éŸ³å¼€å§‹å‰çš„éŸ³é¢‘
        self.pre_buffer_duration_ms = 200  # ç¼“å­˜200msçš„éŸ³é¢‘
        self.pre_buffer = deque(maxlen=int(self.sample_rate * self.pre_buffer_duration_ms / 1000))
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_chunks = 0
        self.voice_chunks = 0
        self.conversation_count = 0
        
        print("âœ… Agoraå¯¹è¯ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def get_state(self) -> ConversationState:
        """è·å–å½“å‰çŠ¶æ€"""
        with self.state_lock:
            return self.state

    def set_state(self, new_state: ConversationState):
        """è®¾ç½®æ–°çŠ¶æ€"""
        with self.state_lock:
            if self.state != new_state:
                print(f"ğŸ”„ çŠ¶æ€åˆ‡æ¢: {self.state.value} â†’ {new_state.value}")
                self.state = new_state

    def process_audio_chunk(self, audio_chunk: np.ndarray):
        """å¤„ç†éŸ³é¢‘å—"""
        self.total_chunks += 1
        current_state = self.get_state()

        # åªåœ¨LISTENINGçŠ¶æ€å¤„ç†VAD
        if current_state != ConversationState.LISTENING:
            return

        # VADæ£€æµ‹
        has_voice = self.vad.process_audio(audio_chunk)

        if has_voice:
            self.voice_chunks += 1

        # æ— è®ºæ˜¯å¦æœ‰è¯­éŸ³ï¼Œéƒ½å¡«å……å‰ç½®ç¼“å†²åŒº
        if not self.is_collecting_speech:
            self.pre_buffer.extend(audio_chunk)

        # VADçŠ¶æ€æœº
        speech_start = False
        speech_end = False

        if has_voice:
            self.speech_chunk_count += 1
            self.silence_chunk_count = 0

            if not self.is_speaking and self.speech_chunk_count >= self.min_speech_chunks:
                self.is_speaking = True
                speech_start = True
        else:
            self.silence_chunk_count += 1
            self.speech_chunk_count = 0

            if self.is_speaking and self.silence_chunk_count >= self.max_silence_chunks:
                self.is_speaking = False
                speech_end = True

        # å¤„ç†è¯­éŸ³äº‹ä»¶
        if speech_start:
            print(f"ğŸ¤ å¼€å§‹ç›‘å¬è¯­éŸ³...")
            self.speech_buffer = []
            self.is_collecting_speech = True
            # å°†å‰ç½®ç¼“å†²åŒºçš„å†…å®¹åŠ å…¥è¯­éŸ³ç¼“å†²åŒºï¼Œä»¥æ•è·å®Œæ•´çš„å¥å­å¼€å¤´
            self.speech_buffer = list(self.pre_buffer)

        if self.is_collecting_speech:
            self.speech_buffer.extend(audio_chunk)

        if speech_end:
            speech_duration = len(self.speech_buffer) / self.sample_rate if self.speech_buffer else 0
            print(f"ğŸ”‡ è¯­éŸ³ç»“æŸï¼Œæ—¶é•¿: {speech_duration:.2f}s")
            self.is_collecting_speech = False

            if len(self.speech_buffer) > self.sample_rate * 0.5:  # æœ€å°è¯­éŸ³æ—¶é•¿ï¼š0.5ç§’
                print(f"ğŸ“ å¼€å§‹è¯­éŸ³è¯†åˆ«...")
                self.set_state(ConversationState.RECOGNIZING)

                # å¼‚æ­¥å¤„ç†ASRå’ŒGPT
                speech_data = np.array(self.speech_buffer, dtype=np.float32)
                threading.Thread(target=self._process_conversation, args=(speech_data,), daemon=True).start()
            else:
                print(f"âš ï¸  è¯­éŸ³å¤ªçŸ­ï¼Œç»§ç»­ç›‘å¬")

    def _process_conversation(self, speech_data: np.ndarray):
        """å¤„ç†å®Œæ•´çš„å¯¹è¯æµç¨‹ï¼ˆASR + GPTï¼‰"""
        try:
            # ASRè¯†åˆ«
            start_time = time.time()
            user_text = self.asr.transcribe_audio_data(speech_data, self.sample_rate)
            asr_time = time.time() - start_time

            if not user_text.strip():
                print("ğŸ”‡ æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³ï¼Œç»§ç»­ç›‘å¬")
                self.set_state(ConversationState.LISTENING)
                return

            print(f"ğŸ“ ç”¨æˆ·è¯´: {user_text} (è¯†åˆ«è€—æ—¶: {asr_time:.2f}s)")

            # GPTå¯¹è¯
            self.set_state(ConversationState.CHATTING)
            print(f"ğŸ¤– æ­£åœ¨æ€è€ƒå›å¤: {user_text}")

            start_time = time.time()
            gpt_reply = self.gpt.chat(user_text)
            gpt_time = time.time() - start_time

            print(f"ğŸ¤– AIå›å¤: {gpt_reply} (æ€è€ƒè€—æ—¶: {gpt_time:.2f}s)")

            # --- ï¼ï¼ï¼åœ¨è¿™é‡Œé›†æˆTTSå’ŒSPEAKINGçŠ¶æ€ !!! ---
            # self.set_state(ConversationState.SPEAKING)
            # print("ğŸ”Š æ­£åœ¨æ’­æ”¾AIå›å¤...")
            #
            # # ä¼ªä»£ç ï¼šè°ƒç”¨TTSå¼•æ“æ’­æ”¾è¯­éŸ³
            # # tts_engine.play(gpt_reply)
            #
            # print("âœ… å›å¤æ’­æ”¾å®Œæˆ")
            # -----------------------------------------

            self.conversation_count += 1

            # å›åˆ°ç›‘å¬çŠ¶æ€
            self.set_state(ConversationState.LISTENING)
            print(f"ğŸ‘‚ ç»§ç»­ç›‘å¬æ–°çš„è¯­éŸ³è¾“å…¥...")

        except Exception as e:
            print(f"âŒ å¯¹è¯å¤„ç†é”™è¯¯: {e}")
            self.set_state(ConversationState.LISTENING)


# ============================================================================
# C++æ¡¥æ¥æ¥å£
# ============================================================================

# å…¨å±€å®ä¾‹
_conversation_system: Optional[AgoraConversationSystem] = None

def bridge_initialize(asr_model_dir: str = None,
                     vad_model_dir: str = None,
                     vad_threshold: float = 0.1, # ç¡®è®¤æ­¤å¤„çš„é»˜è®¤å€¼ä¸º0.1
                     vad_threshold_low: float = 0.005,
                     sample_rate: int = 16000) -> bool:
    """C++è°ƒç”¨çš„åˆå§‹åŒ–å‡½æ•°"""
    global _conversation_system

    try:
        print("ğŸ”§ åˆå§‹åŒ–Agoraå¯¹è¯ç³»ç»Ÿ...")

        # è®¾ç½®é»˜è®¤è·¯å¾„
        if asr_model_dir is None:
            asr_model_dir = "../models/sherpa-onnx-whisper-base.en"
        if vad_model_dir is None:
            vad_model_dir = "../models/snakers4_silero-vad"

        _conversation_system = AgoraConversationSystem(
            asr_model_dir=asr_model_dir,
            vad_model_dir=vad_model_dir,
            vad_threshold=vad_threshold,
            vad_threshold_low=vad_threshold_low,
            sample_rate=sample_rate
        )

        return True

    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def bridge_process_audio(audio_data_ptr: int,
                        data_size: int,
                        samples_per_channel: int,
                        channels: int,
                        sample_rate: int) -> bool:
    """C++è°ƒç”¨çš„éŸ³é¢‘å¤„ç†å‡½æ•° - å¢å¼ºå†…å­˜å®‰å…¨"""
    global _conversation_system

    # ä¸¥æ ¼çš„å‚æ•°æ£€æŸ¥
    if _conversation_system is None:
        return False

    if audio_data_ptr == 0 or data_size <= 0:
        return False

    try:
        # å®‰å…¨åœ°ä»C++æŒ‡é’ˆè¯»å–æ•°æ®
        audio_data = ctypes.string_at(audio_data_ptr, data_size)

        if len(audio_data) != data_size:
            print(f"âš ï¸ æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›{data_size}, å®é™…{len(audio_data)}")
            return False

        # è½¬æ¢éŸ³é¢‘æ•°æ®
        audio_array = np.frombuffer(audio_data, dtype=np.int16)

        # å¤„ç†å¤šå£°é“
        if channels > 1:
            audio_array = audio_array[::channels]

        # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
        if len(audio_array) == 0:
            return False

        # è½¬æ¢ä¸ºfloat32
        audio_float32 = audio_array.astype(np.float32) / 32768.0

        # æ£€æŸ¥æ•°æ®èŒƒå›´
        if np.any(np.isnan(audio_float32)) or np.any(np.isinf(audio_float32)):
            print(f"âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆéŸ³é¢‘æ•°æ®")
            return False

        # å®‰å…¨åœ°å¤„ç†éŸ³é¢‘
        _conversation_system.process_audio_chunk(audio_float32)

        return True

    except (ctypes.ArgumentError, ValueError, MemoryError) as e:
        print(f"âŒ å†…å­˜/æ•°æ®é”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âŒ éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
        return False

def bridge_get_statistics() -> str:
    """C++è°ƒç”¨çš„ç»Ÿè®¡ä¿¡æ¯è·å–å‡½æ•°"""
    global _conversation_system

    if _conversation_system is None:
        return "{}"

    try:
        import json
        voice_ratio = (_conversation_system.voice_chunks / _conversation_system.total_chunks * 100) if _conversation_system.total_chunks > 0 else 0

        stats = {
            'total_chunks': _conversation_system.total_chunks,
            'voice_chunks': _conversation_system.voice_chunks,
            'voice_ratio': voice_ratio,
            'conversation_count': _conversation_system.conversation_count,
            'current_state': _conversation_system.get_state().value
        }

        return json.dumps(stats)

    except Exception as e:
        print(f"âŒ ç»Ÿè®¡è·å–é”™è¯¯: {e}")
        return "{}"

def bridge_shutdown():
    """C++è°ƒç”¨çš„å…³é—­å‡½æ•°"""
    global _conversation_system

    if _conversation_system:
        print("ğŸ›‘ å¯¹è¯ç³»ç»Ÿå…³é—­")
        _conversation_system = None
