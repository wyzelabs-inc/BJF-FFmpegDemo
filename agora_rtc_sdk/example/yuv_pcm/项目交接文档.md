# Agora实时语音对话系统 - 技术交接文档

## 1. 系统功能实现

### 核心功能
本系统实现了一个**实时语音对话AI助手**，具体功能包括：

1. **实时语音监听**: 通过Agora RTC SDK接收音频流
2. **语音活动检测(VAD)**: 自动识别用户何时开始和结束说话
3. **语音转文字(ASR)**: 将用户语音转换为文字
4. **AI对话**: 调用GPT API生成智能回复
5. **状态管理**: 管理整个对话流程的状态切换

### 完整工作流程
```
用户说话 → Agora音频流 → VAD检测语音开始/结束 → 收集语音片段 → ASR识别 → GPT生成回复 → 回到监听状态
```

## 2. C++与Python交互机制

### 2.1 交互架构
- **C++端**: 负责Agora音频流接收和实时处理
- **Python端**: 负责AI模型推理和业务逻辑
- **交互方式**: C++通过Python C API直接调用Python函数

### 2.2 关键交互函数

#### C++调用Python的桥接函数：

1. **`bridge_initialize()`** - 系统初始化
   ```cpp
   // C++调用
   PyObject* result = PyObject_CallFunction(bridge_initialize_func, "ssffi",
       asr_model_dir, vad_model_dir, vad_threshold, vad_threshold_low, sample_rate);
   ```

2. **`bridge_process_audio()`** - 音频数据处理
   ```cpp
   // C++调用
   PyObject* result = PyObject_CallFunction(bridge_process_audio_func, "iiiii",
       (long)audioFrame.buffer,     // 音频数据指针
       dataSize,                    // 数据字节大小
       audioFrame.samplesPerChannel, // 每声道样本数
       audioFrame.channels,         // 声道数
       audioFrame.samplesPerSec);   // 采样率
   ```

3. **`bridge_get_statistics()`** - 获取统计信息
4. **`bridge_shutdown()`** - 系统关闭

### 2.3 数据传递和转换

#### 线程安全与GIL管理
由于Agora的音频回调函数在独立的C++线程中执行，直接调用python C API会因全局解释器锁GIL问题导致程序崩溃(段错误)

**解决方案**
1. **初始化**:在C++主线程初始化python后，调用'PyEval_SaveThread()'释放GIL，允许其他线程获取。
2. **音频处理**:在C++音频回调线程中，调用python函数(如'bridge_process_audio')前，必须使用'PyGILState_Ensure()'获取GIL;调用结束后，使用'PyGILState_Release()'释放GIL。
3. **关闭**: 在主线程调用 `Py_FinalizeEx()` 关闭解释器前，必须重新获取GIL。因为在初始化阶段调用了 `PyEval_SaveThread()`，主线程已经释放了GIL。因此，正确的关闭顺序是先调用 `PyGILState_Ensure()`，再调用 `Py_FinalizeEx()`，这能有效避免程序退出时发生段错误。

#### 音频数据转换流程：
```
Agora AudioFrame (C++) → Python处理
    ↓
1. void* buffer (PCM int16格式)
    ↓
2. ctypes.string_at(audio_data_ptr, data_size)
    ↓
3. np.frombuffer(audio_data, dtype=np.int16)
    ↓
4. 多声道处理: audio_array[::channels] (取左声道)
    ↓
5. 格式转换: audio_array.astype(np.float32) / 32768.0
    ↓
6. 最终格式: numpy float32数组，范围[-1.0, 1.0]
```

#### 关键数据结构：
- **输入**: Agora AudioFrame (16kHz, 16bit PCM)
- **传递**: 内存指针 + 数据大小
- **处理**: numpy float32数组
- **输出**: JSON格式统计信息

## 3. 线程和队列架构

### 3.1 线程结构
系统采用**2线程架构**：

1. **主线程 (C++)**:
   - 运行Agora RTC SDK
   - 接收实时音频流
   - 调用Python处理函数
   - 负责音频数据的实时传递

2. **Python异步线程**:
   - 处理ASR识别 (当检测到完整语音片段时启动)
   - 调用GPT API生成回复
   - 执行`_process_conversation()`函数

### 3.2 队列系统
系统使用**3个主要缓冲区**：

1. **VAD音频缓冲区** (`SileroVAD.audio_buffer`):
   - **作用**: 存储用于VAD分析的音频帧
   - **大小**: 512样本 (32ms@16kHz)
   - **处理**: 滑动窗口，处理完即丢弃,仅用于VAD模型内部

2. **语音收集缓冲区** (`AgoraConversationSystem.speech_buffer`)+前置缓冲区(`pre_buffer`):
   - **作用**: 解决语音开头数据丢失的关键设计。在系统确认语音开始前，持续缓存最近的音频数据
   - **大小**: 动态增长，语音结束时处理
   - **处理**: 当VAD状态机确认语音开始时，此缓冲区内的内容会首先被复制到`speech_buffer`中，确保语音完整性

#### 3.2.1 音频块流向图
1. 下图说明`audio_chunk`如何根据系统状态被送入不同的缓冲区：
```
+---------------------+
|  C++ 接收音频帧     |
+---------------------+
           |
           v
+---------------------+
| Python 接收 audio_chunk |
+---------------------+
           |
           v
+--------------------------------+
| process_audio_chunk()          |
|  (if state != LISTENING: return) |
+--------------------------------+
           |
           v
+--------------------------------+      +--------------------------+
| VAD.process_audio(audio_chunk) |----->| VAD内部缓冲区 (用于分析) |
+--------------------------------+      +--------------------------+
           |
           v
+--------------------------------+
| is_collecting_speech?          |
|      |                 |       |
|     YES                NO      |
|      |                 |       |
|      v                 v       |
| speech_buffer     pre_buffer   |
+--------------------------------+
           |
           v
+--------------------------------+
| VAD状态机判断 speech_start     |
|                                |
| if speech_start:               |
|   speech_buffer = pre_buffer   |
|   is_collecting_speech = True  |
+--------------------------------+
```
2. 下图说明从音频输入到ASR处理的完整流程:
```
+---------------------+
|   接收 audio_chunk   |
+---------------------+
           |
           v
+---------------------+
|  VAD 状态机分析      |
| (has_voice?)        |
+---------------------+
           |
           | speech_start?
           | (连续语音块 > 7)
           v
+--------------------------------+
| 🎤 开始收集语音                  |
| speech_buffer = pre_buffer     |
| is_collecting_speech = True    |
+--------------------------------+
           |
           v
+--------------------------------+
| 持续将 audio_chunk             |
| 添加到 speech_buffer           |
+--------------------------------+
           |
           | speech_end?
           | (连续静音块 > 15)
           v
+--------------------------------+
| 🔇 语音结束                     |
| is_collecting_speech = False   |
+--------------------------------+
           |
           | 语音时长 > 0.5s?
           v
+--------------------------------+
| 启动新线程，将 speech_buffer   |
| (打包成 speech_data)           |
| 传递给 ASR 进行识别            |
+--------------------------------+

```

3. **对话历史缓冲区** (`GPTClient.conversation_history`):
   - **作用**: 存储对话上下文
   - **大小**: 最多10轮对话
   - **处理**: 超出长度时自动截断

### 3.3 状态机详细说明

#### 状态定义：
```python
class ConversationState(Enum):
    LISTENING = "listening"      # 监听语音输入
    RECOGNIZING = "recognizing"  # 语音识别处理中
    CHATTING = "chatting"       # GPT思考回复中
    SPEAKING = "speaking"       # 播放AI回复中(预留)
```

#### 状态转换逻辑：
```
LISTENING → RECOGNIZING: VAD检测到完整语音片段
RECOGNIZING → CHATTING: ASR识别完成，开始GPT处理
CHATTING → LISTENING: GPT回复完成，回到监听状态
```

#### 状态机作用：
1. **防止重复处理**: 只在LISTENING状态处理新音频
2. **资源保护**: 避免同时进行多个ASR/GPT调用
3. **流程控制**: 确保对话流程的顺序执行
4. **状态监控**: 提供系统当前状态的可视化

#### VAD状态机：
```python
# 语音检测状态机
if has_voice:
    speech_chunk_count += 1
    if speech_chunk_count >= min_speech_chunks:
        is_speaking = True  # 语音开始
else:
    silence_chunk_count += 1
    if silence_chunk_count >= max_silence_chunks:
        is_speaking = False  # 语音结束
```

## 4. VAD和ASR参数详解

### 4.1 VAD (语音活动检测) 参数

#### 核心参数：
```python
class SileroVAD:
    def __init__(self,
                 threshold: float = 0.02,      # 语音检测阈值
                 threshold_low: float = 0.005, # 静音检测阈值
                 frame_size: int = 512):       # 分析帧大小
#模型配置 
model_dir = "../models/snakers4_silero-vad"
```

#### 参数说明：
1. **`threshold` (语音阈值, 默认0.1)**:
   - **作用**: VAD模型输出概率超过此值认为是语音
   - **调优**:
     - 调高: 减少误检，但可能漏掉轻声说话
     - 调低: 更敏感，但可能误检环境噪音
 

2. **`threshold_low` (静音阈值, 默认0.005)**:
   - **作用**: VAD概率低于此值认为是静音
   - **调优**: 通常设为threshold的1/4到1/2
   - **作用**: 防止在threshold附近频繁切换状态

3. **`frame_size` (帧大小, 固定512)**:
   - **作用**: 每次VAD分析的音频样本数
   - **时长**: 512样本 = 32ms @ 16kHz
   - **注意**: 不建议修改，Silero VAD模型针对此帧大小优化

#### VAD状态机参数：
```python
class AgoraConversationSystem:
    min_speech_chunks = 7      # 最小连续语音块数
    max_silence_chunks = 15    # 最大连续静音块数
```

4. **`min_speech_chunks` (最小语音块, 默认7)**:
   - **作用**: 连续检测到N块语音才认为语音开始
   - **调优**:
     - 增加: 减少误触发，但增加响应延迟
     - 减少: 更快响应，但可能误触发
   - **延迟**: 每块32ms，1块=32ms延迟

5. **`max_silence_chunks` (最大静音块, 默认15)**:
   - **作用**: 连续检测到N块静音才认为语音结束
   - **调优**:
     - 增加: 允许更长的停顿，适合思考型对话
     - 减少: 更快结束检测，适合快速对话
   - **时长**: 10块 = 480ms静音后结束

### 4.2 ASR (语音识别) 参数

#### 核心参数：
1.  **模型配置**:
    -   `model_dir = "../models/sherpa-onnx-whisper-base.en"`
    -   模型文件需要复制到 `yuv_pcm/models` 目录下才能使用。

2.  **最小语音时长过滤**:
    -   这是一个硬编码在代码中的值，用于过滤掉无意义的短时噪音（如咳嗽、清嗓子等）。
    -   当前值为 **0.5秒**。

#### 参数说明：
1. **最小语音时长 (硬编码值, 当前 0.5秒)**:
   - **作用**: 过滤太短的语音片段，避免无效识别
   - **代码位置**: `agora_vad_asr_gpt_system.py` 的 `process_audio_chunk` 函数中。
     ```python
     if len(self.speech_buffer) > self.sample_rate * 0.5:  # 最小语音时长：0.5秒
         # ... 开始语音识别 ...
     ```
   - **调优**:
     - **调高 (例如 `* 0.8`)**: 可以过滤掉更多类似咳嗽的短促噪音，但可能会漏掉简短的回答（如“好的”）。
     - **调低 (例如 `* 0.2`)**: 可以捕捉到更短的指令，但会增加因噪音误触发ASR和GPT的风险。


## 5. 编译运行和调试

### 5.1 编译步骤
```bash
cd agora_rtc_sdk/example/yuv_pcm/build
cmake ..
make vad_asr_gpt_intergrated
cp ../agora_vad_asr_system.py ./
```

### 5.2 运行程序
```bash
./vad_asr_gpt_intergrated --token pull_token(用main_push.py生成) --channelId JS_TEST1 --userId 999 --remoteUserId 1 --vadThreshold 0.1 --vadThresholdLow 0.005
```

### 5.3 关键日志解读
```bash
# 正常启动日志
✅ PyTorch可用
✅ Sherpa-ONNX可用
✅ OpenAI可用
🔧 初始化Agora对话系统...
✅ Silero VAD模型加载成功
✅ GPT客户端初始化成功

# 运行时日志
🎤 开始监听语音...                    # VAD检测到语音开始
🔇 语音结束，时长: 1.31s              # VAD检测到语音结束
📝 用户说: Hello world (识别耗时: 0.68s)  # ASR识别结果
🤖 AI回复: Hi there! (思考耗时: 3.2s)     # GPT回复结果
🔄 状态切换: listening → recognizing      # 状态机切换
```

### 5.4 统计信息监控
```json
{
  "total_chunks": 1000,        // 总处理音频块数
  "voice_chunks": 300,         // 检测到语音的块数
  "voice_ratio": 30.0,         // 语音比例(%)
  "conversation_count": 5,     // 完成的对话轮数
  "current_state": "listening" // 当前系统状态
}
```

---
